---
title: Quick Start
description: Deploy Smallest Self-Host Speech-to-Text with Docker Compose in under 15 minutes
---

## Overview

This guide walks you through deploying Smallest Self-Host using Docker Compose. You'll have a fully functional speech-to-text service running in under 15 minutes.

<Note>
  Ensure you've completed all [prerequisites](/docker/stt/prerequisites) before
  starting this guide.
</Note>

## Choose Your Model

Smallest Self-Host supports multiple model variants optimized for different use cases:

<CardGroup cols={2}>
  <Card title="Lightning ASR" icon="bolt">
    **Fastest inference**
    
    - Real-time factor: 0.05-0.10x
    - GPU memory: ~10 GB
    - Best for: High-volume, real-time
    - Setup: Requires MODEL_URL
  </Card>

  <Card title="Lightning V2" icon="sparkles">
    **Latest version**
    
    - Real-time factor: 0.08-0.12x
    - GPU memory: ~12 GB
    - Best for: Improved accuracy
    - Setup: Model embedded
  </Card>

</CardGroup>

<Tip>
  **Not sure which to choose?** Start with **Lightning V2** for the best balance
  of speed and accuracy. It's easier to set up (no MODEL_URL needed) and
  provides excellent results for most use cases.
</Tip>

## Step 1: Create Project Directory

Create a directory for your deployment:

```bash
mkdir -p ~/smallest-self-host
cd ~/smallest-self-host
```

## Step 2: Login to Container Registry

Authenticate with the Smallest container registry using credentials provided by support:

```bash
docker login quay.io
```

Enter your username and password when prompted.

<Tip>
  Save your credentials securely. You'll need them if you restart or redeploy
  the containers.
</Tip>

## Step 3: Create Environment File

Create a `.env` file with your license key:

```bash
cat > .env << 'EOF'
LICENSE_KEY=your-license-key-here
EOF
```

Replace `your-license-key-here` with the actual license key provided by Smallest.ai.

<Warning>
  Never commit your `.env` file to version control. Add it to `.gitignore` if
  using git.
</Warning>

## Step 4: Create Docker Compose File

Choose the configuration that matches your model type:

<Tabs>
  <Tab title="Lightning ASR (Standard)">
    **Best for:** Fast inference, real-time applications
    
    Create a `docker-compose.yml` file:

    ```yaml docker-compose.yml
    version: "3.8"

    services:
      lightning-asr:
        image: quay.io/smallestinc/lightning-asr:latest
        ports:
          - "2233:2233"
        environment:
          - MODEL_URL=${MODEL_URL}
          - LICENSE_KEY=${LICENSE_KEY}
          - REDIS_URL=redis://redis:6379
          - PORT=2233
        deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  count: 1
                  capabilities: [gpu]
        restart: unless-stopped
        networks:
          - smallest-network

      api-server:
        image: quay.io/smallestinc/self-hosted-api-server:latest
        container_name: api-server
        environment:
          - LICENSE_KEY=${LICENSE_KEY}
          - LIGHTNING_ASR_BASE_URL=http://lightning-asr:2233
          - API_BASE_URL=http://license-proxy:3369
        ports:
          - "7100:7100"
        networks:
          - smallest-network
        restart: unless-stopped
        depends_on:
          - lightning-asr
          - license-proxy

      license-proxy:
        image: quay.io/smallestinc/license-proxy:latest
        container_name: license-proxy
        environment:
          - LICENSE_KEY=${LICENSE_KEY}
        networks:
          - smallest-network
        restart: unless-stopped

      redis:
        image: redis:7-alpine
        ports:
          - "6379:6379"
        networks:
          - smallest-network
        restart: unless-stopped
        command: redis-server --appendonly yes
        healthcheck:
          test: ["CMD", "redis-cli", "ping"]
          interval: 5s
          timeout: 3s
          retries: 5

    networks:
      smallest-network:
        driver: bridge
        name: smallest-network
    ```

  </Tab>

  <Tab title="Lightning V2">
    **Best for:** Latest model version with improved accuracy
    
    Create a `docker-compose.yml` file:

    ```yaml docker-compose.yml
    version: "3.8"

    services:
      lightning-v2:
        image: quay.io/smallestinc/lightning-v2:latest
        container_name: lightning-v2
        ports:
          - "2269:2269"
        environment:
          - PORT=2269
          - LICENSE_KEY=${LICENSE_KEY}
          - REDIS_HOST=redis
          - REDIS_PORT=6379
        deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  count: 1
                  capabilities: [gpu]
        restart: unless-stopped
        networks:
          - smallest-network

      api-server:
        image: quay.io/smallestinc/self-hosted-api-server:latest
        container_name: api-server
        environment:
          - PORT=7100
          - LICENSE_KEY=${LICENSE_KEY}
          - LIGHTNING_V2_BASE_URL=http://lightning-v2:2269
          - API_BASE_URL=http://license-proxy:3369
          - REDIS_HOST=redis
          - REDIS_PORT=6379
        ports:
          - "7100:7100"
        networks:
          - smallest-network
        restart: unless-stopped
        depends_on:
          - lightning-v2
          - license-proxy

      license-proxy:
        image: quay.io/smallestinc/license-proxy:latest
        container_name: license-proxy
        environment:
          - PORT=3369
          - LICENSE_KEY=${LICENSE_KEY}
        networks:
          - smallest-network
        restart: unless-stopped

      redis:
        image: redis:7-alpine
        container_name: redis-server
        ports:
          - "6379:6379"
        networks:
          - smallest-network
        restart: unless-stopped
        command: redis-server --appendonly yes
        volumes:
          - redis-data:/data

    networks:
      smallest-network:
        driver: bridge
        name: smallest-network

    volumes:
      redis-data:
    ```

  </Tab>
</Tabs>

## Step 5: Additional Configuration for Lightning ASR

<Tabs>
  <Tab title="Lightning ASR">
    Add the model URL to your `.env` file (required for Lightning ASR):

    ```bash
    echo "MODEL_URL=your-model-url-here" >> .env
    ```

    The MODEL_URL is provided by Smallest.ai support.

  </Tab>
</Tabs>

## Step 6: Start Services

Launch all services with Docker Compose:

```bash
docker compose up -d
```

<Tabs>
  <Tab title="Lightning ASR - First Startup">
    First startup will take 5-10 minutes as the system:
    1. Pulls container images (~5 GB)
    2. Downloads ASR models (~20 GB)
    3. Initializes GPU and loads models
  </Tab>

  <Tab title="Lightning V2 / Large - First Startup">
    First startup will take 3-5 minutes as the system:
    1. Pulls container images (~15-25 GB, includes models)
    2. Initializes GPU and loads models
    
    Models are embedded in the container - no separate download needed.
  </Tab>

  <Tab title="Subsequent Startups">
    After the first run, startup takes 30-60 seconds for all model types as images are cached.
  </Tab>
</Tabs>

## Step 7: Monitor Startup

Watch the logs to monitor startup progress:

```bash
docker compose logs -f
```

Look for these success indicators:

<Steps>
  <Step title="Redis Ready">
    ```
    redis-1  | Ready to accept connections
    ```
  </Step>

  <Step title="License Proxy Ready">
    ```
    license-proxy  | License validated successfully
    license-proxy  | Server listening on port 3369
    ```
  </Step>

  <Step title="Model Service Ready">
    **Lightning ASR:**
    ```
    lightning-asr-1  | Model loaded successfully
    lightning-asr-1  | Server ready on port 2233
    ```
    
    **Lightning V2:**
    ```
    lightning-v2  | Model loaded successfully
    lightning-v2  | Server ready on port 2269
    ```
    
    **Lightning Large:**
    ```
    lightning-large  | Model loaded successfully
    lightning-large  | Server ready on port 9989
    ```
  </Step>

  <Step title="API Server Ready">
    ```
    api-server  | Connected to Lightning ASR
    api-server  | API server listening on port 7100
    ```
  </Step>
</Steps>

Press `Ctrl+C` to stop following logs.

## Step 8: Test API

Test the API with a sample request:

```bash
curl -X POST http://localhost:7100/v1/listen \
  -H "Authorization: Token ${LICENSE_KEY}" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://example.com/sample-audio.wav"
  }'
```

<Tip>
Replace the URL with your own audio file, or use the health check endpoint first:

```bash
curl http://localhost:7100/health
```

Expected response: `{"status": "healthy"}`

</Tip>

## Common Startup Issues

<AccordionGroup>
  <Accordion title="GPU Not Found" icon="triangle-exclamation">
    **Error:** `could not select device driver "nvidia"`
    
    **Solution:**
    ```bash
    sudo systemctl restart docker
    docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi
    ```
    
    If this fails, reinstall NVIDIA Container Toolkit.
  </Accordion>

  <Accordion title="License Validation Failed" icon="key">
    **Error:** `License validation failed`
    
    **Solution:**
    - Verify LICENSE_KEY in `.env` is correct
    - Check internet connectivity
    - Ensure firewall allows HTTPS to console-api.smallest.ai
  </Accordion>

  <Accordion title="Model Download Failed" icon="download">
    **Error:** `Failed to download model`
    
    **Solution:**
    - Verify MODEL_URL in `.env` is correct
    - Check disk space: `df -h`
    - Check internet connectivity
  </Accordion>

  <Accordion title="Port Already in Use" icon="network-wired">
    **Error:** `port is already allocated`
    
    **Solution:**
    Check what's using the port:
    ```bash
    sudo lsof -i :7100
    ```
    
    Either stop the conflicting service or change the port in docker-compose.yml
  </Accordion>
</AccordionGroup>

## Managing Your Deployment

### Stop Services

```bash
docker compose stop
```

### Restart Services

```bash
docker compose restart
```

### View Logs

```bash
docker compose logs -f [service-name]
```

Examples:

```bash
docker compose logs -f api-server
docker compose logs -f lightning-asr
```

### Update Images

Pull latest images and restart:

```bash
docker compose pull
docker compose up -d
```

### Remove Deployment

Stop and remove all containers:

```bash
docker compose down
```

Remove containers and volumes (including downloaded models):

```bash
docker compose down -v
```

<Warning>
  Using `-v` flag will delete all data including downloaded models. They will
  need to be re-downloaded on next startup.
</Warning>

## What's Next?

<CardGroup cols={2}>
  <Card title="STT Configuration" icon="gear" href="/docker/stt/configuration">
    Customize your deployment with advanced configuration options
  </Card>

  <Card title="STT Services Overview" icon="layer-group" href="/docker/stt/services-overview">
    Learn about each service component in detail
  </Card>

  <Card title="STT Troubleshooting" icon="wrench" href="/docker/stt/docker-troubleshooting">
    Debug common issues and optimize performance
  </Card>

  <Card title="API Reference" icon="code" href="/api-reference/authentication">
    Integrate with your applications using the API
  </Card>
</CardGroup>
